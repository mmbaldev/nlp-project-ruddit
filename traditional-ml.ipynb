{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle savings helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "\n",
    "def save_data(object_name, object):\n",
    "    \"\"\"\n",
    "    Save the given object to a pickle file.\n",
    "\n",
    "    Parameters:\n",
    "    object_name (str): The name of the object.\n",
    "    object: The object to be saved.\n",
    "    \"\"\"\n",
    "    with open(f'./data/pickles/{object_name}.pickle', 'wb') as f:\n",
    "        pickle.dump(object, f)\n",
    "\n",
    "def load_data(object_name, calculator, *args):\n",
    "    \"\"\"\n",
    "    Load the data from a pickle file. If the file doesn't exist, calculate the data using the given calculator function,\n",
    "    save it to a pickle file, and return the calculated data.\n",
    "\n",
    "    Parameters:\n",
    "    object_name (str): The name of the object.\n",
    "    calculator (function): The function to calculate the data if the file doesn't exist.\n",
    "    *args: Additional arguments to be passed to the calculator function.\n",
    "\n",
    "    Returns:\n",
    "    The loaded or calculated data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(f'./data/pickles/{object_name}.pickle', 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        obj = calculator(*args)\n",
    "        save_data(object_name, obj)\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "PATH = './data/ruddit.csv'\n",
    "TRAIN_RATIO = 0.75\n",
    "TEST_VAL_RATIO = 1\n",
    "\n",
    "dataset = pd.read_csv(PATH)\n",
    "x_train, x_test_valid, y_train, y_test_valid = train_test_split(dataset[\"comment_text\"], dataset['offensiveness_score'] , train_size=TRAIN_RATIO, random_state=0)\n",
    "x_test, x_valid, y_test, y_valid = train_test_split(x_test_valid, y_test_valid, test_size=TEST_VAL_RATIO, random_state=0)\n",
    "x_train = x_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "x_test = x_test.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "x_valid = x_valid.reset_index(drop=True)\n",
    "y_valid = y_valid.reset_index(drop=True)\n",
    "train_raw = pd.DataFrame({'text': x_train, 'score': y_train})\n",
    "test_raw = pd.DataFrame({'text': x_test, 'score': y_test})\n",
    "valid_raw = pd.DataFrame({'text': x_valid, 'score': y_valid})\n",
    "train_raw['score'] = train_raw['score'].astype('float32')\n",
    "test_raw['score'] = test_raw['score'].astype('float32')\n",
    "valid_raw['score'] = valid_raw['score'].astype('float32')\n",
    "del x_train, x_test, x_valid, y_train, y_test, y_valid, x_test_valid, y_test_valid\n",
    "len(train_raw), len(test_raw), len(valid_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    pattern = re.compile('[^a-zA-Z]')\n",
    "    text = pattern.sub(' ', text)\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "    text = [wl.lemmatize(word) for word in text if not word in stop_words]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "def clean_dataset(ds):\n",
    "    ds['text'] = ds['text'].apply(preprocess_text)\n",
    "    return ds\n",
    "\n",
    "train = clean_dataset(train_raw.copy())\n",
    "test = clean_dataset(test_raw.copy())\n",
    "valid = clean_dataset(valid_raw.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "pca = PCA(n_components=300)\n",
    "bow_train = vectorizer.fit_transform(train_raw['text']).toarray()\n",
    "bow_test = vectorizer.fit_transform(test_raw['text']).toarray()\n",
    "bow_valid = vectorizer.fit_transform(valid_raw['text']).toarray()\n",
    "\n",
    "pad_width = ((0, 0), (0, bow_train.shape[1] - bow_test.shape[1])) \n",
    "bow_test = np.pad(bow_test, pad_width, mode='constant', constant_values=0)\n",
    "pad_width = ((0, 0), (0, bow_train.shape[1] - bow_valid.shape[1])) \n",
    "bow_valid = np.pad(bow_valid, pad_width, mode='constant', constant_values=0)\n",
    "\n",
    "bow_train = pca.fit_transform(bow_train)\n",
    "bow_test = pca.fit_transform(bow_test)\n",
    "# bow_valid = pca.fit_transform(bow_valid)\n",
    "\n",
    "\n",
    "bow_train.shape, bow_test.shape, bow_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_train = vectorizer.fit_transform(train_raw['text']).toarray()\n",
    "tfidf_test = vectorizer.fit_transform(test_raw['text']).toarray()\n",
    "tfidf_valid = vectorizer.fit_transform(valid_raw['text']).toarray()\n",
    "\n",
    "pad_width = ((0, 0), (0, tfidf_train.shape[1] - tfidf_test.shape[1])) \n",
    "tfidf_test = np.pad(tfidf_test, pad_width, mode='constant', constant_values=0)\n",
    "pad_width = ((0, 0), (0, tfidf_train.shape[1] - tfidf_valid.shape[1])) \n",
    "tfidf_valid = np.pad(tfidf_valid, pad_width, mode='constant', constant_values=0)\n",
    "\n",
    "tfidf_train = pca.fit_transform(tfidf_train)\n",
    "tfidf_test = pca.fit_transform(tfidf_test)\n",
    "\n",
    "tfidf_train.shape, tfidf_test.shape, tfidf_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api \n",
    "\n",
    "w2vec_google_news_model = load_data('w2vec_google_news_model', api.load, 'word2vec-google-news-300')\n",
    "glove_twitter_model = load_data('glove_twitter_model', api.load, 'glove-wiki-gigaword-300')\n",
    "fasttext_wiki_news_model = load_data('fasttext_wiki_news_model', api.load, 'fasttext-wiki-news-subwords-300')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = [text.split() for text in train_raw['text']]\n",
    "tokenized_test = [text.split() for text in test_raw['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(sentence_tokens, w2v_model, vector_size=300):\n",
    "    words_vecs = [w2v_model[word] for word in sentence_tokens if word in w2v_model]\n",
    "    if len(words_vecs) == 0:\n",
    "        return np.zeros(vector_size)\n",
    "    words_vecs = np.array(words_vecs)\n",
    "    return words_vecs.mean(axis=0)\n",
    "\n",
    "train_fasttext = ([vectorize(tokens, fasttext_wiki_news_model) for tokens in tokenized_train])\n",
    "test_fasttext = ([vectorize(tokens, fasttext_wiki_news_model) for tokens in tokenized_test])\n",
    "train_v2w = ([vectorize(tokens, w2vec_google_news_model) for tokens in tokenized_train])\n",
    "test_v2w = ([vectorize(tokens, w2vec_google_news_model) for tokens in tokenized_test])\n",
    "train_glove = ([vectorize(tokens, glove_twitter_model) for tokens in tokenized_train])\n",
    "test_glove = ([vectorize(tokens, glove_twitter_model) for tokens in tokenized_test])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMs representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def bert_embeddings(ds):\n",
    "    # Tokenize the texts\n",
    "    tokenized_inputs = tokenizer(list(ds), truncation=True, padding=True, return_tensors=\"pt\").to(device)\n",
    "    tokenized_inputs.to('cuda')\n",
    "    # Generate BERT embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**tokenized_inputs)\n",
    "\n",
    "    # Extract the embeddings\n",
    "    embeddings = model_output.last_hidden_state\n",
    "    embeddings = embeddings.cpu()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return embeddings.mean(dim=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_embeddings_cuda(ds):\n",
    "    batch_size = 100  # You can adjust this according to your GPU memory capacity\n",
    "\n",
    "    # Initialize a list to store embeddings for each text\n",
    "    embeddings_list = []\n",
    "    final_embeddings = torch.tensor([])\n",
    "    # Process texts in batches\n",
    "    for i in range(0, len(ds), batch_size):\n",
    "        batch_texts = ds[i:i+batch_size]\n",
    "        batch_tokens = tokenizer(batch_texts.tolist(), return_tensors='pt', padding=True, truncation=True) \n",
    "        batch_tokens.to(device)        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch_tokens)\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        last_hidden_states = last_hidden_states.cpu().numpy()\n",
    "        embeddings = np.mean(last_hidden_states, axis=1)\n",
    "        final_embeddings = torch.cat((final_embeddings, torch.tensor(embeddings)), 0)\n",
    "        # Append the embeddings to the list\n",
    "        # embeddings_list.append(last_hidden_states)\n",
    "        \n",
    "        # Empty CUDA memory\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return final_embeddings.numpy()\n",
    "    # Convert the embeddings list to a numpy array\n",
    "    # embeddings_array = np.concatenate(embeddings_list, axis=0)\n",
    "    # return torch.tensor(embeddings_array).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bert = bert_embeddings_cuda(train_raw['text'])\n",
    "test_bert = bert_embeddings_cuda(test_raw['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bert.shape, test_bert.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_bert = bert_embeddings(train['text'])\n",
    "# test_bert = bert_embeddings(test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    'bow': (bow_train, bow_test, bow_valid),\n",
    "    'tfidf': (tfidf_train, tfidf_test, tfidf_valid),\n",
    "    'fasttext': (train_fasttext, test_fasttext),\n",
    "    'word2vec': (train_v2w, test_v2w),\n",
    "    'glove': (train_glove, test_glove),\n",
    "    'bert': (train_bert, test_bert)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "linear_reg = LinearRegression()\n",
    "svr_reg = SVR(kernel = 'rbf')\n",
    "mlp_reg = MLPRegressor(random_state=1, max_iter=500)\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train = tfidf_train\n",
    "model_test = tfidf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_reg.fit(model_train, train['score'])\n",
    "svr_reg.fit(model_train, train['score'])\n",
    "mlp_reg.fit(model_train, train['score'])\n",
    "rf_reg.fit(model_train, train['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_preds = svr_reg.predict(model_test)\n",
    "linear_preds = linear_reg.predict(model_test)\n",
    "mlp_preds = mlp_reg.predict(model_test)\n",
    "rf_preds = rf_reg.predict(model_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, classification_report\n",
    "svr_mse = mean_squared_error(test['score'], svr_preds)\n",
    "linear_mse = mean_squared_error(test['score'], linear_preds)\n",
    "mlp_mse = mean_squared_error(test['score'], mlp_preds)\n",
    "rf_mse = mean_squared_error(test['score'], rf_preds)\n",
    "print(f'SVR MSE: {svr_mse}, Linear MSE: {linear_mse}, MLP MSE: {mlp_mse}, RF MSE: {rf_mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_clean = {\n",
    "    'fasttext': 50.6,\n",
    "    'word2vec': 48.9,\n",
    "    'glove': 49.4,\n",
    "    'bert_embeddings': 113\n",
    "}\n",
    "times_dirty = {\n",
    "    'fasttext': 55.7,\n",
    "    'word2vec': 54.3,\n",
    "    'glove': 54.2,\n",
    "    'bert_embeddings': 117\n",
    "}\n",
    "results_mse_cleaned = {\n",
    "    'bow': {'SVR': 0.12376995930427524, 'Linear': 0.17338464620420832, 'MLP': 0.2564379379779361, 'RF': 0.1131010225230955},\n",
    "    'tfidf': {'SVR': 0.12007776170149795, 'Linear': 0.1410922185815179, 'MLP': 0.22373554702562642, 'RF': 0.10625150627656584},\n",
    "    'fasttext': {'SVR': 0.04685816665354699, 'Linear': 0.07273326632161381, 'MLP': 0.05643423252877395, 'RF': 0.06449617951793207},\n",
    "    'Word2Vec': {'SVR': 0.04816666309223599, 'Linear': 0.07068408441559317, 'MLP': 0.0655118452881107, 'RF': 0.07014625622457209},\n",
    "    'Glove': {'SVR': 0.05030112790450738, 'Linear': 0.0782962824967885, 'MLP': 0.07334035831215925, 'RF': 0.07286072318841706},\n",
    "    'bert_embeddings': {'SVR': 0.05263384180285866, 'Linear': 0.058188606053590775, 'MLP': 0.07364223152399063, 'RF': 0.06752015332228646}\n",
    "}\n",
    "results_mse_dirty = {\n",
    "    'bow': {'SVR': 0.12816722954426424, 'Linear': 0.1646148816792815, 'MLP': 0.21052087127847655, 'RF': 0.11326946991419015},\n",
    "    'tfidf': {'SVR': 0.13030425877084306, 'Linear': 0.17210081402548744, 'MLP': 0.23246958937586945, 'RF': 0.11589168438060171,},\n",
    "    'fasttext': {'SVR': 0.058821879162422326, 'Linear': 0.08009510252213108, 'MLP': 0.08096923470973573, 'RF': 0.07731245249116338},\n",
    "    'Word2Vec': {'SVR': 0.05957748697334124, 'Linear': 0.08946030570644542, 'MLP': 0.09698301239891266, 'RF': 0.08042184426460584},\n",
    "    'Glove': {'SVR': 0.07340861310598293, 'Linear': 0.09466922426057676, 'MLP': 0.10379193202544125, 'RF': 0.08534953326234156},\n",
    "    'bert_embeddings':{'SVR': 0.04606831641743619, 'Linear': 0.05261010304093361, 'MLP': 0.06015215069055557, 'RF': 0.0667452847313885}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = pd.DataFrame(results_mse_cleaned)\n",
    "df_dirty = pd.DataFrame(results_mse_dirty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter('output.xlsx') as writer:\n",
    "    df_cleaned.to_excel(writer, sheet_name='Results_Cleaned')\n",
    "    df_dirty.to_excel(writer, sheet_name='Results_Dirty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
