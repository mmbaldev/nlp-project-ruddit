{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mehdi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/mehdi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\\\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           comment_text  \\\n",
      "0     > The difference in average earnings between m...   \n",
      "1     The myth is that the \"gap\" is entirely based o...   \n",
      "2     The assertion is that women get paid less for ...   \n",
      "3     You said in the OP that's not what they're mea...   \n",
      "4     >Men and women are not payed less for the same...   \n",
      "...                                                 ...   \n",
      "5629  They should only censor things that talk badly...   \n",
      "5630  > and one of them is a woman. \\n\\nOH SHIT we b...   \n",
      "5631                  how is this flared as US politics   \n",
      "5632  People in Hong Kong must decide if they are go...   \n",
      "5633  I know this is an old post but I saw him last ...   \n",
      "\n",
      "                                             post_title  \\\n",
      "0              CMV: The so called \"Wage Gap\" is a myth.   \n",
      "1              CMV: The so called \"Wage Gap\" is a myth.   \n",
      "2              CMV: The so called \"Wage Gap\" is a myth.   \n",
      "3              CMV: The so called \"Wage Gap\" is a myth.   \n",
      "4              CMV: The so called \"Wage Gap\" is a myth.   \n",
      "...                                                 ...   \n",
      "5629                      Don't let china censor Reddit   \n",
      "5630               THE PURGE IS HAPPENING IN HONG KONG!   \n",
      "5631                      Don't let china censor Reddit   \n",
      "5632               THE PURGE IS HAPPENING IN HONG KONG!   \n",
      "5633  I saw this man with a sign in Washington yeste...   \n",
      "\n",
      "                                              post_text  offensiveness_score  \n",
      "0                                             [removed]               -0.083  \n",
      "1                                             [removed]               -0.022  \n",
      "2                                             [removed]               -0.146  \n",
      "3                                             [removed]               -0.083  \n",
      "4                                             [removed]               -0.042  \n",
      "...                                                 ...                  ...  \n",
      "5629                                                NaN                0.064  \n",
      "5630  Please support Hong Kong and let the world see...                0.458  \n",
      "5631                                                NaN               -0.292  \n",
      "5632  Please support Hong Kong and let the world see...                0.333  \n",
      "5633                                                NaN               -0.625  \n",
      "\n",
      "[5634 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = pd.read_csv('./data/ruddit.csv')\n",
    "print(dataset)\n",
    "sentences = dataset['comment_text']\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataset['comment_text'], dataset['offensiveness_score'], train_size=0.75, test_size=0.25, random_state=0)\n",
    "x_train = x_train.reset_index(drop=True)\n",
    "x_test = x_test.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_corpus(corpus):\n",
    "    tokenized_clean_docs = []\n",
    "    cleaned_docs = []\n",
    "    for doc in corpus:\n",
    "        text_data = re.sub('[^a-zA-Z]', ' ', doc)\n",
    "        text_data = text_data.lower()\n",
    "        text_data = text_data.split()\n",
    "        wl = WordNetLemmatizer()\n",
    "        text_data = [wl.lemmatize(word) for word in text_data if not word in set(stopwords.words('english'))]\n",
    "        text_data = ' '.join(text_data)\n",
    "        cleaned_docs.append(text_data)\n",
    "        tokenized_clean_docs.append(word_tokenize(text_data))\n",
    "\n",
    "    cleaned_docs = pd.Series(cleaned_docs)\n",
    "    return cleaned_docs, tokenized_clean_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource 'corpora/wordnet' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/home/mehdi/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/corpus/util.py:80\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzip_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/data.py:653\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    652\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (sep, msg, sep)\n\u001b[0;32m--> 653\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/wordnet.zip/wordnet/.zip/' not found.  Please\n  use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - '/home/mehdi/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x_train_cleaned, x_train_tokenized \u001b[38;5;241m=\u001b[39m \u001b[43mclean_corpus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m x_test_cleaned, x_test_tokenized \u001b[38;5;241m=\u001b[39m clean_corpus(x_test)\n",
      "Cell \u001b[0;32mIn [7], line 9\u001b[0m, in \u001b[0;36mclean_corpus\u001b[0;34m(corpus)\u001b[0m\n\u001b[1;32m      7\u001b[0m text_data \u001b[38;5;241m=\u001b[39m text_data\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m      8\u001b[0m wl \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[0;32m----> 9\u001b[0m text_data \u001b[38;5;241m=\u001b[39m [wl\u001b[38;5;241m.\u001b[39mlemmatize(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m text_data \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))]\n\u001b[1;32m     10\u001b[0m text_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(text_data)\n\u001b[1;32m     11\u001b[0m cleaned_docs\u001b[38;5;241m.\u001b[39mappend(text_data)\n",
      "Cell \u001b[0;32mIn [7], line 9\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m text_data \u001b[38;5;241m=\u001b[39m text_data\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m      8\u001b[0m wl \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[0;32m----> 9\u001b[0m text_data \u001b[38;5;241m=\u001b[39m [\u001b[43mwl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m text_data \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))]\n\u001b[1;32m     10\u001b[0m text_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(text_data)\n\u001b[1;32m     11\u001b[0m cleaned_docs\u001b[38;5;241m.\u001b[39mappend(text_data)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/stem/wordnet.py:40\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize\u001b[39m(\u001b[38;5;28mself\u001b[39m, word, pos\u001b[38;5;241m=\u001b[39mNOUN):\n\u001b[0;32m---> 40\u001b[0m     lemmas \u001b[38;5;241m=\u001b[39m \u001b[43mwordnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_morphy\u001b[49m(word, pos)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(lemmas, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m lemmas \u001b[38;5;28;01melse\u001b[39;00m word\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/corpus/util.py:116\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m: root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir, zip_name))\n\u001b[0;32m---> 81\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/corpus/util.py:78\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m: root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir, zip_name))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/data.py:653\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    651\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    652\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (sep, msg, sep)\n\u001b[0;32m--> 653\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/wordnet' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/home/mehdi/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************"
     ]
    }
   ],
   "source": [
    "x_train_cleaned, x_train_tokenized = clean_corpus(x_train)\n",
    "x_test_cleaned, x_test_tokenized = clean_corpus(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "# Use the fit_transform method to transform the sentences into a bag of words\n",
    "bow = vectorizer.fit_transform(cleaned_sentences)\n",
    "# Print the vocabulary (features) of the bag of words\n",
    "print(vectorizer.get_feature_names())\n",
    "# Print the bag of words\n",
    "print(bow.toarray())\n",
    "print(bow.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Create a TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "# Use the fit_transform method to transform the documents into a TF-IDF matrix\n",
    "tfidf = vectorizer.fit_transform(sentences)\n",
    "# Print the vocabulary (features) of the TF-IDF matrix\n",
    "print(vectorizer.get_feature_names())\n",
    "# Print the TF-IDF matrix\n",
    "print(tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the dataset for a w2v model using gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api \n",
    "\n",
    "w2v = Word2Vec(tokenized_clean_sentences, min_count=1,vector_size=300)\n",
    "\n",
    "print(w2v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads gensim pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(api.info()['models'].keys()))\n",
    "w2vec_google_news_model = api.load('word2vec-google-news-300')\n",
    "glove_twitter_model = api.load('glove-twitter-200')\n",
    "fasttext_wiki_news_model = api.load('fasttext-wiki-news-subwords-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words that are not in the embedding model vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_not_in_vocab(sentences_tokens, w2v_model):\n",
    "    not_in_words = []\n",
    "    for sentence_tokens in sentences_tokens:\n",
    "        for word in sentence_tokens:\n",
    "            if word not in w2v_model and word not in not_in_words:\n",
    "                not_in_words.append(word)\n",
    "    return not_in_words\n",
    "\n",
    "print(len(words_not_in_vocab(tokenized_clean_sentences, w2vec_google_news_model)))\n",
    "print(len(words_not_in_vocab(tokenized_clean_sentences, glove_twitter_model)))\n",
    "print(len(words_not_in_vocab(tokenized_clean_sentences, fasttext_wiki_news_model)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: retrain the fasttext model with vocabularies that are not existed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A vectorize method for calculate the w2v of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(sentence_tokens, w2v_model, vector_size=300):\n",
    "    words_vecs = [w2v_model[word] for word in sentence_tokens if word in w2v_model]\n",
    "    if len(words_vecs) == 0:\n",
    "        return np.zeros(vector_size)\n",
    "    words_vecs = np.array(words_vecs)\n",
    "    return words_vecs.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_vectorized = ([vectorize(tokens, fasttext_wiki_news_model) for tokens in x_train_tokenized])\n",
    "x_test_vectorized = ([vectorize(tokens, fasttext_wiki_news_model) for tokens in x_test_tokenized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# clf = LogisticRegression()\n",
    "# clf.fit(x_train_vectorized, y_train)\n",
    "\n",
    "linear_reg = LinearRegression()\n",
    "linear_reg.fit(x_train_vectorized, y_train)\n",
    "\n",
    "\n",
    "svr_reg = SVR(kernel = 'rbf')\n",
    "svr_reg.fit(x_train_vectorized, y_train)\n",
    "\n",
    "mlp_reg = MLPRegressor(random_state=1, max_iter=500)\n",
    "mlp_reg.fit(x_train_vectorized, y_train)\n",
    "\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_reg.fit(x_train_vectorized, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "def model_inference(sentence, model):\n",
    "    tokens = sentence.split()\n",
    "    sentence_embedding = vectorize(tokens, fasttext_wiki_news_model)\n",
    "    return model.predict([sentence_embedding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_pred = clf.predict(x_test_vectorized)\n",
    "svr_preds = svr_reg.predict(x_test_vectorized)\n",
    "linear_preds = linear_reg.predict(x_test_vectorized)\n",
    "mlp_preds = mlp_reg.predict(x_test_vectorized)\n",
    "rf_preds = rf_reg.predict(x_test_vectorized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, classification_report\n",
    "SVR_MAE = mean_absolute_error(y_test, svr_preds)\n",
    "SVR_MSE = mean_squared_error(y_test, svr_preds)\n",
    "REG_MAE = mean_absolute_error(y_test, linear_preds)\n",
    "REG_MSE = mean_squared_error(y_test, linear_preds)\n",
    "MLP_MAE = mean_absolute_error(y_test, mlp_preds)\n",
    "MLP_MSE = mean_squared_error(y_test, mlp_preds)\n",
    "RF_MAE = mean_absolute_error(y_test, rf_preds)\n",
    "RF_MSE = mean_squared_error(y_test, rf_preds)\n",
    "\n",
    "print(f\"SVR MAE score is: {SVR_MAE}\")\n",
    "print(f\"SVR MSE score is: {SVR_MSE}\")\n",
    "print(svr_reg.score(x_test_vectorized, y_test))\n",
    "print(f\"REG MAE score is: {REG_MAE}\")\n",
    "print(f\"REG MSE score is: {REG_MSE}\")\n",
    "print(linear_reg.score(x_test_vectorized, y_test))\n",
    "print(f\"MLP MAE score is: {MLP_MAE}\")\n",
    "print(f\"MLP MSE score is: {MLP_MSE}\")\n",
    "print(mlp_reg.score(x_test_vectorized, y_test))\n",
    "print(f\"RF MAE score is: {RF_MAE}\")\n",
    "print(f\"RF MSE score is: {RF_MSE}\")\n",
    "print(rf_reg.score(x_test_vectorized, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.plot(y_test[:100])\n",
    "plt.plot(svr_preds[:100])\n",
    "plt.plot(linear_preds[:100])\n",
    "plt.plot(mlp_preds[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_inference(\"youre fucking nice \", regressor)\n",
    "# for index, pred in enumerate(svr_preds):\n",
    "    # if (pred - y_test[index])**2 > 0.5:\n",
    "    #     print(x_test[index], pred, y_test[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Prepreation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('./data/ruddit.csv')\n",
    "x_train, x_test_valid, y_train, y_test_valid = train_test_split(dataset[\"comment_text\"], dataset['offensiveness_score'] , train_size=0.8, test_size=0.2, random_state=0)\n",
    "x_test, x_valid, y_test, y_valid = train_test_split(x_test_valid, y_test_valid, test_size=0.5, random_state=0)\n",
    "x_train = x_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "x_test = x_test.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "x_valid = x_valid.reset_index(drop=True)\n",
    "y_valid = y_valid.reset_index(drop=True)\n",
    "train = pd.DataFrame({'text': x_train, 'score': y_train})\n",
    "test = pd.DataFrame({'text': x_test, 'score': y_test})\n",
    "valid = pd.DataFrame({'text': x_valid, 'score': y_valid})\n",
    "train['score'] = train['score'].astype('float32')\n",
    "test['score'] = test['score'].astype('float32')\n",
    "valid['score'] = valid['score'].astype('float32')\n",
    "print(valid.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BASE_MODEL = \"bert-base-cased\"\n",
    "LEARNING_RATE = 2e-5\n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 2\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(list(train['text']), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(list(valid['text']), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(list(test['text']), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class RudditDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = RudditDataset(train_encodings, train['score'])\n",
    "val_dataset = RudditDataset(val_encodings, valid['score'])\n",
    "test_dataset = RudditDataset(test_encodings, test['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def compute_metrics_for_regression(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    labels = labels.reshape(-1, 1)\n",
    "    \n",
    "    mse = mean_squared_error(labels, logits)\n",
    "    mae = mean_absolute_error(labels, logits)\n",
    "    r2 = r2_score(labels, logits)\n",
    "    single_squared_errors = ((logits - labels).flatten()**2).tolist()\n",
    "    \n",
    "    # Compute accuracy \n",
    "    # Based on the fact that the rounded score = true score only if |single_squared_errors| < 0.5\n",
    "    accuracy = sum([1 for e in single_squared_errors if e < 0.25]) / len(single_squared_errors)\n",
    "    \n",
    "    return {\"mse\": mse, \"mae\": mae, \"r2\": r2, \"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/fine-tuned-regression-2\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    load_best_model_at_end=True,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class RegressionTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs[0][:, 0]\n",
    "        loss = torch.nn.functional.mse_loss(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = RegressionTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics_for_regression,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.eval_dataset=test_dataset\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mehdi/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/mehdi/.local/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/mehdi/.local/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "for i in x_test:\n",
    "    lengths.append(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195.50319375443576\n",
      "908\n",
      "14\n",
      "171.70949487011723\n",
      "450.4000000000001\n",
      "579.7999999999997\n",
      "744.4400000000005\n"
     ]
    }
   ],
   "source": [
    "lengths = np.array(lengths)\n",
    "print(lengths.mean())\n",
    "print(lengths.max())\n",
    "print(lengths.min())\n",
    "print(lengths.std())\n",
    "print(np.percentile(lengths, 90))\n",
    "print(np.percentile(lengths, 95))\n",
    "print(np.percentile(lengths, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_statistics(dataset_df, labels=True):\n",
    "    tokens_count = 0\n",
    "    min_tokens = 10000000\n",
    "    max_tokens = 0\n",
    "    labels_count = []\n",
    "    for index, row in dataset_df.iterrows():\n",
    "        print(index, end='\\r')\n",
    "        sentence_tokens_count = len(word_tokenize(row['text']))\n",
    "        tokens_count += sentence_tokens_count\n",
    "        if sentence_tokens_count > max_tokens:\n",
    "            max_tokens = sentence_tokens_count\n",
    "        if sentence_tokens_count < min_tokens:\n",
    "            min_tokens = sentence_tokens_count\n",
    "        if 'labels' in row and labels:\n",
    "            labels_count.append(len(row['labels']))\n",
    "    print(\"Number of documents: \", len(dataset_df))\n",
    "    print(\"Average number of tokens: \", tokens_count/len(dataset_df))\n",
    "    print(\"Max number of tokens: \", max_tokens)\n",
    "    print(\"Min number of tokens: \", min_tokens)\n",
    "    if labels:\n",
    "        print(\"Average number of labels: \", sum(labels_count)/len(labels_count))\n",
    "        print(\"Max number of labels: \", max(labels_count))\n",
    "        print(\"Min number of labels: \", min(labels_count))\n",
    "        print(\"Number of documents with no labels: \", labels_count.count(0))\n",
    "\n",
    "    # Plot top 15 aspects\n",
    "    if 'labels' in dataset_df:\n",
    "        all_labels = [label for sublist in dataset_df['labels'] for label in sublist]\n",
    "        label_counts = pd.Series(all_labels).value_counts()\n",
    "        top_15_labels = label_counts.head(15)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        top_15_labels.plot(kind='bar', color='skyblue')\n",
    "        plt.title('Top 15 Aspects Distributions')\n",
    "        plt.xlabel('Aspects')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
